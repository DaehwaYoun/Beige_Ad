{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "import csv\n",
    "import pickle\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_count(T_count, CDS_count, CDSlen):\n",
    "    avg_CDS_count = CDS_count / CDSlen\n",
    "    \n",
    "    return {key: value/avg_CDS_count for key, value in T_count.items()}\n",
    "\n",
    "\n",
    "def calc_avg_density_range(direc, SP, site_set, P_gtf, codon_pos_info, scope):\n",
    "    print(f'{SP} {site_set} start -----')\n",
    "    \n",
    "    valid_ID = set(P_gtf.keys())\n",
    "    P_gtf[''] = [0,0]\n",
    "    \n",
    "    T_norm_counts = {}\n",
    "    T_ID = ''\n",
    "    T_count = {}\n",
    "    CDS_count = -1\n",
    "    with gzip.open(f'{direc}/{SP}.rep.tagcoords.gz', 'rt') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            if line[1] not in valid_ID : continue\n",
    "            # if information of a transcript ends\n",
    "            if T_ID != line[1] :\n",
    "                # save normalized counts of the transcript, if it passes criterion; at least 1 CDS count per codon\n",
    "                if CDS_count >= P_gtf[T_ID][1]/3 : T_norm_counts[T_ID] = norm_count(T_count, CDS_count, P_gtf[T_ID][1])\n",
    "                # then reset\n",
    "                T_count = {}\n",
    "                CDS_count = 0\n",
    "            T_ID = line[1]\n",
    "            \n",
    "            # counting\n",
    "            if line[4]=='CDS' : CDS_count += 1\n",
    "            if T_count.get(int(line[2]))==None : T_count[int(line[2])] = 0\n",
    "            T_count[int(line[2])] += 1\n",
    "    \n",
    "    sum_band = [0.]*(scope*2+3)\n",
    "    num_band = 0\n",
    "    for T_ID in T_norm_counts:\n",
    "        T_norm_count = T_norm_counts[T_ID]\n",
    "        if codon_pos_info.get(T_ID)==None : continue\n",
    "        for pos in codon_pos_info[T_ID]:\n",
    "            band = [0.]*(scope*2+3)\n",
    "            focus = pos-scope\n",
    "            for i in range(len(band)):\n",
    "                if T_norm_count.get(focus)!=None : band[i] = T_norm_count[focus]\n",
    "                focus += 1\n",
    "            \n",
    "            sum_band = [sum(x) for x in zip(sum_band, band)]\n",
    "            num_band += 1\n",
    "    \n",
    "    avg_band = [x/num_band for x in sum_band]\n",
    "    \n",
    "    print(f'{SP} {site_set} done  +++++')\n",
    "    \n",
    "    return [SP, site_set, avg_band]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D0a D0a_Top5% start -----\n",
      "D0a D0a_Mid5% start -----\n",
      "D0a D0a_Btm5% start -----\n",
      "D0b D0b_Top5% start -----\n",
      "D0b D0b_Mid5% start -----\n",
      "D0b D0b_Btm5% start -----\n",
      "D0c D0c_Top5% start -----\n",
      "D0c D0c_Mid5% start -----\n",
      "D0c D0c_Btm5% start -----\n",
      "D4a D4a_Top5% start -----\n",
      "D4a D4a_Mid5% start -----\n",
      "D4a D4a_Btm5% start -----\n",
      "D4b D4b_Top5% start -----\n",
      "D4b D4b_Mid5% start -----\n",
      "D4b D4b_Btm5% start -----\n",
      "D4c D4c_Top5% start -----\n",
      "D4c D4c_Mid5% start -----\n",
      "D4c D4c_Btm5% start -----\n",
      "D8a D8a_Top5% start -----\n",
      "D8a D8a_Mid5% start -----\n",
      "D8a D8a_Btm5% start -----\n",
      "D8b D8b_Top5% start -----\n",
      "D8b D8b_Mid5% start -----\n",
      "D8b D8b_Btm5% start -----\n",
      "D8c D8c_Top5% start -----\n",
      "D8c D8c_Mid5% start -----\n",
      "D8c D8c_Btm5% start -----\n",
      "D0b D0b_Mid5% done  +++++\n",
      "D0a D0a_Top5% done  +++++\n",
      "D0a D0a_Mid5% done  +++++\n",
      "D0b D0b_Top5% done  +++++\n",
      "D0b D0b_Btm5% done  +++++\n",
      "D0a D0a_Btm5% done  +++++\n",
      "D0c D0c_Top5% done  +++++\n",
      "D0c D0c_Mid5% done  +++++\n",
      "D8a D8a_Mid5% done  +++++\n",
      "D8a D8a_Top5% done  +++++\n",
      "D0c D0c_Btm5% done  +++++\n",
      "D8a D8a_Btm5% done  +++++\n",
      "D8c D8c_Btm5% done  +++++\n",
      "D8c D8c_Mid5% done  +++++\n",
      "D8c D8c_Top5% done  +++++\n",
      "D4a D4a_Btm5% done  +++++\n",
      "D8b D8b_Mid5% done  +++++\n",
      "D8b D8b_Top5% done  +++++\n",
      "D4b D4b_Top5% done  +++++\n",
      "D4b D4b_Mid5% done  +++++\n",
      "D4b D4b_Btm5% done  +++++\n",
      "D4c D4c_Btm5% done  +++++\n",
      "D8b D8b_Btm5% done  +++++\n",
      "D4a D4a_Top5% done  +++++\n",
      "D4a D4a_Mid5% done  +++++\n",
      "D4c D4c_Mid5% done  +++++\n",
      "D4c D4c_Top5% done  +++++\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    threads = 36\n",
    "    \n",
    "    direc = '/Data_2/Jun/Adipocytes/rpf/novaseq'\n",
    "    SPs = [day+rep for day in ['D0','D4','D8'] for rep in ['a','b','c']]\n",
    "    scope = 150\n",
    "    \n",
    "\n",
    "    # Loading stalling site info.\n",
    "    with open('/Data_2/Daehwa/Adipocyte/Analysis/Ribosome_stalling/v20230730/adi_stall-score.df.pickle',\"rb\") as fr: stalling_score = pickle.load(fr)\n",
    "    stalling_score['AA_codon'] = stalling_score['aa-asite']+' '+stalling_score['codon-asite']\n",
    "    stalling_score = stalling_score.replace(0, np.nan).dropna()\n",
    "\n",
    "    stalling_codon_pos_infos = {}\n",
    "    for SP in stalling_score.iloc[:,5:-1]:\n",
    "        # Top 5%\n",
    "        tmp = stalling_score.sort_values(SP, ascending=False)[:(len(stalling_score)//100)*5]\n",
    "        \n",
    "        stalling_codon_pos_infos[f'{SP}_Top5%'] = {}\n",
    "        for index, row in tmp[['transcript_id','asite']].iterrows():\n",
    "            if stalling_codon_pos_infos[f'{SP}_Top5%'].get(row[0])==None : stalling_codon_pos_infos[f'{SP}_Top5%'][row[0]] = []\n",
    "            stalling_codon_pos_infos[f'{SP}_Top5%'][row[0]].append(row[1])\n",
    "\n",
    "        # Mid 5%\n",
    "        tmp = stalling_score.sort_values(SP, ascending=False)[(len(stalling_score)//1000)*475:(len(stalling_score)//1000)*525]\n",
    "        \n",
    "        stalling_codon_pos_infos[f'{SP}_Mid5%'] = {}\n",
    "        for index, row in tmp[['transcript_id','asite']].iterrows():\n",
    "            if stalling_codon_pos_infos[f'{SP}_Mid5%'].get(row[0])==None : stalling_codon_pos_infos[f'{SP}_Mid5%'][row[0]] = []\n",
    "            stalling_codon_pos_infos[f'{SP}_Mid5%'][row[0]].append(row[1])\n",
    "\n",
    "        # Near1\n",
    "        tmp = stalling_score.sort_values(SP, ascending=False)[(len(stalling_score)//100)*60:(len(stalling_score)//100)*65]\n",
    "        \n",
    "        stalling_codon_pos_infos[f'{SP}_Near1'] = {}\n",
    "        for index, row in tmp[['transcript_id','asite']].iterrows():\n",
    "            if stalling_codon_pos_infos[f'{SP}_Near1'].get(row[0])==None : stalling_codon_pos_infos[f'{SP}_Near1'][row[0]] = []\n",
    "            stalling_codon_pos_infos[f'{SP}_Near1'][row[0]].append(row[1])\n",
    "\n",
    "        # Bottom 5%\n",
    "        tmp = stalling_score.sort_values(SP, ascending=False)[-(len(stalling_score)//100)*5:]\n",
    "        \n",
    "        stalling_codon_pos_infos[f'{SP}_Btm5%'] = {}\n",
    "        for index, row in tmp[['transcript_id','asite']].iterrows():\n",
    "            if stalling_codon_pos_infos[f'{SP}_Btm5%'].get(row[0])==None : stalling_codon_pos_infos[f'{SP}_Btm5%'][row[0]] = []\n",
    "            stalling_codon_pos_infos[f'{SP}_Btm5%'][row[0]].append(row[1])\n",
    "        \n",
    "    # display(stalling_codon_pos_infos)\n",
    "\n",
    "\n",
    "    # Loading GTF info.\n",
    "    P_gtf = pd.read_csv('/Data_2/Daehwa/Data_Library/GTF_parsed/v0.7.1/gencode.vM27.annotation.gtf/Processed_gtf.tsv',\n",
    "                        sep='\\t', usecols=['Transcript_ID','Transcript_length','CDS_length'])\n",
    "    P_gtf = P_gtf[P_gtf[\"CDS_length\"]>0]\n",
    "    P_gtf = P_gtf[P_gtf[\"CDS_length\"]%3==0]\n",
    "    P_gtf = P_gtf.set_index('Transcript_ID').T.to_dict('list')\n",
    "    \n",
    "    # Run the analysis\n",
    "    args = [(direc, site_set[:3], site_set, P_gtf, stalling_codon_pos_infos[site_set], scope) for site_set in stalling_codon_pos_infos]\n",
    "    p = Pool(processes=threads)\n",
    "    results = p.starmap(calc_avg_density_range, args)\n",
    "    p.close()\n",
    "    \n",
    "    # organization of results\n",
    "    result_org = {TB:{SP:None for SP in SPs} for TB in ['Top5%','Mid5%','Near1','Btm5%']}\n",
    "    for result in results:\n",
    "        result_org[result[1][-5:]][result[0]] = result[2]\n",
    "    \n",
    "    # Save as \n",
    "    out_w = pd.ExcelWriter(f'adi_top-stalling-score_metagene.xlsx')\n",
    "    for TB in result_org:\n",
    "        pd.DataFrame(result_org[TB]).to_excel(out_w, sheet_name=TB, index=False)\n",
    "    out_w.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
